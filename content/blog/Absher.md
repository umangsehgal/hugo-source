---
author: "Umang Sehgal"
date: 2019-04-20
title: Case Study - Would you promote an app that tracks your lady?
image: image/theme/absher.jpg 
---


Suppose you wake up one morning to find yourself barricaded in a hotel room in the Bangkok airport to avoid being returned to your family. That is when you had slipped away from your family during a holiday in Kuwait and boarded a plane for Thailand, but were stopped in the airport because your “male guardian” received a text message on his phone about your escape notified by the app Absher. Such was the case of Rahaf Alqunun who amongst many other women had the app impede her attempt to flee from an abusive home environment.

What alarms human rights activists, is how male guardians can use the app to specify when and where women may travel, for how long and which airports they're allowed to go to. Alerts get triggered if a woman leaves a certain area and Absher notifies male guardians while fleeing women is arrested. It is this access to private data and location which can have women face death at the hands of their family.

Within this controversy against the tech giants hosting this app on their respective app-stores, there’s also a famous opposition form the critics. A lot of women who want to run away have been able to use it to run away by sneaking the phone away from their guardians, and accessing the grant permissions in the app. While this is a temporary solution, one can’t ignore the Saudi monarchy’s restriction and repression on women and diverge from the kind of data being produced, what can be done with that data and where can this data be used to harm lives of abused (or innocent) women. 
	
While we argue that one’s privacy is a matter of context and that privacy is lost when information flows in ways that violate understood or context-based norms. The app defines privacy on a continuum, defining how it helps Saudi women understand and navigate their social lives. This difference in context-based privacy is vastly different across different countries that smudges the boundary between guardianship and gender apartheid. Despite the app not violating any rules of the respective app-stores, what becomes difficult is these definitions of government enabled apps in promoting abuse through access to data and mobility enforcement through it. 

So, what? We know that the culture there promotes Saudi men to take control of their women even without the existence of the app. Campaigns defending Saudis have detailed that the app is inextricably bound up with other features of the app like checking mail, registering vehicles, and applying for visas. It is important to realize that these power structures that are pre-built and the data being shared by the app takes it for granted that controlling women mobility is for intrinsic good.

As the technologies integrate and interrelate, search data and social network analysis could make it even further impossible for women to be able to escape such brutality. The current perspectives from which the app is allowed to be hosted assumes that most of those people can only control mobility when a passport is scanned at the borders, which is an oversimplification of how this technology adoption can open avenue to a multitude of ways to track women both directly and indirectly. 

Rahaf Alqunun represented a very common approach to the gaming the app and planning an escape. Her escape and the adoption of this app by the two tech giants open so many avenues to challenge the state of personal privacy and challenge the surveillance model created by the government. So far, the world has debated privacy cases where people do stuff that generates data and it usually means that those who come to possess that data should be able to do whatever they want with it, but with Absher, the data is neither being generated at will and nor is it being shared at will. 

It isn’t just about regulating the information available to the “male guardians” but it’s also about how this data is made actionable at places like Airports. It’s also about how the government and authorities react or respond to certain kind of information and which brings us to the fact that privacy isn’t solely a matter of individual responsibility or preference. Rightly said, privacy is a set of conditions enacted in practice, propped up by any combination of history, laws, social norms, physical or digital structures, context, individual behavior, and happenstance. Limiting data privacy to just one of these forces (like the airport border scan) risks misunderstanding it entirely.

When we evaluate this with reference to human rights and a desire to minimize intervention into people’s lives, another question that naturally crops up is how do we move away from just such individual-based concerns and foreground social and structural impacts that are at the root of gender apartheid in Saudi Arabia. The question extends, how do we achieve a state where we think and create more careful conditions to tackle ethical problems being empowered through these data-intensive applications?

Finally, what is it that we as everyday programmers can learn and pay attention to from this? How do we ensure that there are algorithms in place to check for ethicality of an action before it is reported and becomes a popular media issue. In many ways, this trap is unavoidable. We as programmers are particularly skilled at one thing: optimization. But optimization alone can't tell us anything about the morality of what we are optimizing for. As the data about these women grows, the more difficult would it be to rule out cases that identify misuse of data and powers attached to it. 

Though extreme, this example proves a key point: locational tracking data alone doesn't contain the potential needed to assess the fairness or justness of a given system. In order for the tech companies to understand the misuse of this data would be by not limiting their thinking about fairness and justice to internal logic of a given system, without reference to broader social or political realities. 

According to Kranzberg’s fourth law, “Technologically ‘sweet’ solutions do not always triumph over political and social forces.” While empowering citizens with quick access to services, the app will be upholding the society’s weakness of keeping women in check.  Finally, software that is so inherently directed at the suppression of fundamental rights, such as the right to travel, must have the organizations treat it as a violation of the Universal Declaration of Human Rights. While tech is neither good nor bad, its non-neutrality indicates that the problems with data originate from political dogmas. 



