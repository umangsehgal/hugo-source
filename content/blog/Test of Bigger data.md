---
author: "Umang Sehgal"
date: 2019-02-20
title: The Test of Bigger Data!
---

# The Test of Bigger Data

Cathy O'Neil's (2017) article goes by the subheading “With computers as with humans, abundant information can reinforce prejudice”, the author claims that handing data over to statistical algorithms that are trained on historical data is not likely to improve human prejudice in making decisions. As students of science we have always been told that more data is always better in terms of linear models. Why is it that we observe contrasting behaviors leading to a higher prejudice even in large datasets? 

Cathy’s article takes a field experiment conducted by Harvard Business Review (Rivera and Tilcsik, 2016) that involved sending 316 fake applications to the top law firms in the U.S.  The fake applicants were all among the top 1 percent of tier 2 universities, however their last names, college clubs/positions and hobbies were varied to provide indirect information about their gender and social class. 

It is to be observed that big data is not always equivalent of whole data (boyd & crawford, 2012) and regardless of the size of experiment, these biased algorithms showed upper-class males were four times as likely to get a callback as other candidates, including upper-class women. This suggested that even among equally qualified candidates, the added information gave the biased algorithm something not to promote, such as a lower-class background, or the possibility that a woman might decide to have children and leave the firm.

One could always question that, “Are the data representative of all applications?” to support the ethical nature of the algorithms in place. Not really, because they might exclude applications from employee referrals and other sources. Although, it is imperative to note that - Are the data representative of all public applications? Perhaps, but not necessarily (boyd & crawford, 2012).

When researchers comment on the ethical nature of a system, they need to publicly account for not only the limits of the training dataset, but also the limits of which questions they can ask of a dataset and what interpretations are appropriate (boyd & crawford, 2012). While the article does talk about feedback from the attorneys they surveyed (Rivera and Tilcsik, 2016), it is equally important to pose the limits of these interpretations for us as readers to gain a vivid comparison between the ethical bias of the screening system in contrast to the human interpretations.

What remains overlooked is bias of certain factors used in this ‘big data’ to examine the success of candidate applications. Data driven algorithms tend to add larger weights to certain factors depending on how they are hyper tuned in a particular system. This fails to prove a direct correlation between social factors and success determination, which could in some instances hurt the ethical claims we make. 

Finally, the study concludes that using less data would be better by removing the information on gender and social clubs altogether and focusing on performance in law school as a solution to neutralize the ethical dis-orientation we face. But is that really the right way – going forward? 


## Reference List
Cathy O'Neil. (2017, March 1). Bigger Data Isn't Always Better Data - Bloomberg. Retrieved February 3, 2019, from https://www.bloomberg.com/opinion/articles/2017-03-01/bigger-data-isn-t-always-better-data
(2016, December 21). Research: How Subtle Class Cues Can Backfire on Your Resume. Retrieved February 3, 2019, from https://hbr.org/2016/12/research-how-subtle-class-cues-can-backfire-on-your-resume
Boyd, D., & Crawford, K. (2012). Critical questions for big data: Provocations for a cultural, technological, and scholarly phenomenon. Information, communication & society, 15(5), 662-679.
(n.d.). Timing Your Application - Cornell Law School - Cornell University. Retrieved February 3, 2019, from https://www.lawschool.cornell.edu/careers/students/apply_job/app_timing.cfm

